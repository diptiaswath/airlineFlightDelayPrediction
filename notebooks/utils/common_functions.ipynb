{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb81fce6",
   "metadata": {},
   "source": [
    "\n",
    "## Utility Functions used in Notebooks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee90fd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc, classification_report, confusion_matrix, accuracy_score, recall_score, precision_score, make_scorer, average_precision_score, precision_recall_curve, roc_curve, f1_score, roc_auc_score, ConfusionMatrixDisplay,make_scorer, RocCurveDisplay, PrecisionRecallDisplay\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel, SequentialFeatureSelector, RFE\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2acd31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################################################################\n",
    "# Generic Function to cap outliers in a specific column of input dataframe to the given lower and upper bounds \n",
    "####################################################################################################################################################\n",
    "def cap_outliers(df_cleaned, column, lower_bound, upper_bound):\n",
    "    \"\"\"\n",
    "    Cap outliers in a specified column of input DataFrame to the given bounds.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas DataFrame\n",
    "    - column: str, name of the column to process\n",
    "    - lower_bound: float, lower bound for capping\n",
    "    - upper_bound: float, upper bound for capping\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with outliers capped\n",
    "    \"\"\"\n",
    "    print(f\"\\nColumn {column} has outliers greater than upper bound ({upper_bound}) or lower than lower bound ({lower_bound}). Capping them now.\")\n",
    "    df_cleaned[column] = df_cleaned[column].clip(lower=lower_bound, upper=upper_bound)\n",
    "    return df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f8c1563",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################################################################\n",
    "# Generic Function to identify, display and drop duplicate rows in input dataframe\n",
    "####################################################################################################################################################\n",
    "def handle_duplicates(df):\n",
    "    \"\"\"\n",
    "    Identifies, displays, counts, and drops duplicate rows in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas DataFrame\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with duplicate rows removed\n",
    "    \"\"\"\n",
    "    # Identify duplicate rows\n",
    "    duplicate_rows = df.duplicated()\n",
    "    \n",
    "    # Display duplicate rows\n",
    "    duplicates = df[duplicate_rows]\n",
    "    print(\"\\nActual Duplicate Rows:\")\n",
    "    print(duplicates)\n",
    "    \n",
    "    # Count duplicate rows\n",
    "    duplicate_count = duplicate_rows.sum()\n",
    "    print(\"\\nNumber of Duplicate Rows:\", duplicate_count)\n",
    "    \n",
    "    # Drop duplicate rows\n",
    "    df_cleaned = df.drop_duplicates()\n",
    "    print(\"\\nAfter dropping duplicate rows, the count of duplicate rows now: \", df_cleaned.duplicated().sum())\n",
    "    \n",
    "    return df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4049347f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################\n",
    "# Generic Model Evaluation function to get additional metrics used across all classifiers\n",
    "###################################################################################################\n",
    "\n",
    "# Generic Function to get additional metrics given an estimator, test feature set and target labels\n",
    "def evaluate_model(estimator, Xt_test, yt_test, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Evaluates a fitted decision tree pipeline using various performance metrics, including\n",
    "    Precision-Recall, ROC AUC curves, and confusion matrix, and displays plots.\n",
    "\n",
    "    Parameters:\n",
    "    estimator (Model): The fitted model.\n",
    "    Xt_test (pd.DataFrame): The test feature set.\n",
    "    yt_test (pd.Series): The test target labels.\n",
    "    threshold: Threshold.\n",
    "    Returns:\n",
    "    dict: A dictionary with various evaluation metrics and confusion matrix details.\n",
    "    \"\"\"\n",
    "    # Predict class labels for multiclass prediction\n",
    "    y_pred = estimator.predict(Xt_test)\n",
    "\n",
    "    # Predict probabilities for multiclass classification\n",
    "    y_pred_proba = estimator.predict_proba(Xt_test)\n",
    "\n",
    "    # Compute F1 score on test set\n",
    "    test_f1 = f1_score(yt_test, y_pred, average='macro')\n",
    "    test_f1_weighted = f1_score(yt_test, y_pred, average='weighted')\n",
    "\n",
    "    # Compute Accuracy score on test set\n",
    "    test_accuracy = accuracy_score(yt_test, y_pred)\n",
    "\n",
    "    # Classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(yt_test, y_pred))\n",
    "\n",
    "    # Compute PR AUC and ROC AUC using one-vs-rest for each class\n",
    "    pr_auc_values = []\n",
    "    roc_auc_values = []\n",
    "\n",
    "    for cls in np.unique(yt_test):\n",
    "        # Convert to binary: 1 if the class is `cls`, 0 otherwise\n",
    "        y_test_binary = (yt_test == cls).astype(int)\n",
    "\n",
    "        # Get class-specific predicted scores (log-probabilities for PR AUC and ROC AUC)\n",
    "        y_score_proba = y_pred_proba[:, cls]\n",
    "\n",
    "        # Precision-Recall AUC for class `cls`\n",
    "        pr_auc_value = average_precision_score(y_test_binary, y_score_proba)\n",
    "        pr_auc_values.append(pr_auc_value)\n",
    "        print(f\"PR AUC for class {cls}: {pr_auc_value:.2f}\")\n",
    "\n",
    "        # ROC AUC for class `cls`\n",
    "        fpr, tpr, _ = roc_curve(y_test_binary, y_score_proba)\n",
    "        roc_auc_value = auc(fpr, tpr)\n",
    "        roc_auc_values.append(roc_auc_value)\n",
    "        print(f\"ROC AUC for class {cls}: {roc_auc_value:.2f}\")\n",
    "\n",
    "    # Macro-averaged PR AUC (treat each class equally)\n",
    "    pr_auc_macro = np.mean(pr_auc_values)\n",
    "    print(f\"Macro-averaged PR AUC: {pr_auc_macro:.2f}\")\n",
    "\n",
    "    # Macro-averaged ROC AUC\n",
    "    roc_auc_macro = np.mean(roc_auc_values)\n",
    "    print(f\"Macro-averaged ROC AUC: {roc_auc_macro:.2f}\")\n",
    "\n",
    "    # Compute the weighted average PR AUC score\n",
    "    class_counts = np.bincount(yt_test)\n",
    "    pr_auc_weighted = np.average(pr_auc_values, weights=class_counts)\n",
    "    print(f\"Weighted PR AUC Score: {pr_auc_weighted:.2f}\")\n",
    "\n",
    "    # Compute roc_auc_score with 'weighted' averaging\n",
    "    roc_auc_weighted = roc_auc_score(yt_test, y_pred_proba, average='weighted', multi_class='ovr')\n",
    "    print(f\"Weighted ROC AUC Score: {roc_auc_weighted:.2f}\")\n",
    "\n",
    "    # Plot subplots\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "    # Plot1. Confusion Matrix for Multiclass Classification\n",
    "    conf_matrix = confusion_matrix(yt_test, y_pred)\n",
    "    disp1 = ConfusionMatrixDisplay(conf_matrix)\n",
    "    disp1.plot(ax=ax[0])\n",
    "    ax[0].set_title('Confusion Matrix')\n",
    "\n",
    "    # Plot2. Precision-Recall Curves for each class (One-vs-Rest)\n",
    "    for cls in np.unique(yt_test):\n",
    "        # Binarize the true labels and predictions for the current class\n",
    "        yt_test_binary = (yt_test == cls).astype(int)\n",
    "        y_pred_proba_cls = y_pred_proba[:, cls]\n",
    "\n",
    "        # Calculate Precision-Recall curve\n",
    "        precision, recall, _ = precision_recall_curve(yt_test_binary, y_pred_proba_cls)\n",
    "\n",
    "        # Plot Precision-Recall curve\n",
    "        disp2 = PrecisionRecallDisplay(precision=precision, recall=recall)\n",
    "        disp2.plot(ax=ax[1], name=f'Class {cls} (PR AUC = {pr_auc_values[cls]:.2f})')\n",
    "\n",
    "    ax[1].set_title('Precision-Recall Curves (One-vs-Rest)')\n",
    "\n",
    "    # Plot3. ROC Curves for each class (One-vs-Rest)\n",
    "    for cls in np.unique(yt_test):\n",
    "        # Binarize the true labels and predictions for the current class\n",
    "        yt_test_binary = (yt_test == cls).astype(int)\n",
    "        y_pred_proba_cls = y_pred_proba[:, cls]\n",
    "\n",
    "        # Calculate ROC curve\n",
    "        fpr, tpr, _ = roc_curve(yt_test_binary, y_pred_proba_cls)\n",
    "\n",
    "        # Plot ROC curve\n",
    "        disp3 = RocCurveDisplay(fpr=fpr, tpr=tpr)\n",
    "        disp3.plot(ax=ax[2], name=f'Class {cls} (ROC AUC = {roc_auc_values[cls]:.2f})')\n",
    "\n",
    "    ax[2].set_title('ROC AUC Curves (One-vs-Rest)')\n",
    "\n",
    "    # Adjust layout and show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Return evaluation metrics in a dictionary\n",
    "    results = {\n",
    "        'f1_score_macro'   : test_f1,\n",
    "        'f1_score_weighted': test_f1_weighted,\n",
    "        'accuracy_score'  : test_accuracy,\n",
    "        'pr_auc_macro'    : pr_auc_macro,\n",
    "        'roc_auc_macro'   : roc_auc_macro,\n",
    "        'pr_auc_weighted' : pr_auc_weighted,\n",
    "        'roc_auc_weighted': roc_auc_weighted,\n",
    "        'confusion_matrix': conf_matrix\n",
    "    }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1838d43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################\n",
    "# Generic Function to analyze Feature Selection results from input Pipeline \n",
    "###################################################################################################\n",
    "def analyze_feature_selection(pipeline):\n",
    "    \"\"\"\n",
    "    Analyze feature selection results from a pipeline.\n",
    "\n",
    "    Parameters:\n",
    "    pipeline (sklearn.pipeline.Pipeline): A fitted pipeline containing a feature selection step.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing two elements:\n",
    "           1. A list of tuples (feature_name, rank) for all features, sorted by rank.\n",
    "           2. A list of selected feature names.\n",
    "    \"\"\"\n",
    "    def get_feature_names(column_transformer):\n",
    "        feature_names = []\n",
    "        for name, transformer, columns in column_transformer.transformers_:\n",
    "            if name == 'passthrough':\n",
    "                feature_names.extend(columns)\n",
    "            elif hasattr(transformer, 'get_feature_names_out'):\n",
    "                feature_names.extend(transformer.get_feature_names_out(input_features=columns))\n",
    "            else:\n",
    "                feature_names.extend(columns)\n",
    "        return feature_names\n",
    "\n",
    "    # Get feature names from the preprocessor\n",
    "    feature_names = get_feature_names(pipeline.named_steps['preprocessor'])\n",
    "\n",
    "    # Get feature selector\n",
    "    feature_selector = pipeline.named_steps['feature_selection']\n",
    "\n",
    "    # Get selected indices\n",
    "    selected_indices = feature_selector.get_support(indices=True)\n",
    "\n",
    "    # Initialize feature ranking\n",
    "    feature_ranking = []\n",
    "    # Get feature ranking if available\n",
    "    if hasattr(feature_selector, 'ranking_'):\n",
    "        ranking = feature_selector.ranking_\n",
    "        feature_ranking = list(zip(feature_names, ranking))\n",
    "        feature_ranking.sort(key=lambda x: x[1])\n",
    "\n",
    "    # Verify the length of selected_indices and feature_names\n",
    "    if max(selected_indices) >= len(feature_names):\n",
    "        raise ValueError(f\"Error: Selected indices {selected_indices} exceed the number of features ({len(feature_names)}).\")\n",
    "\n",
    "    # Map selected indices to feature names\n",
    "    selected_feature_names = [feature_names[i] for i in selected_indices]\n",
    "\n",
    "    return feature_ranking, selected_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e81ad7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################\n",
    "# Generic Function to plot feature importances from a trained classifier\n",
    "###################################################################################################\n",
    "def plot_feature_importances(classifier, selected_features):\n",
    "    \"\"\"\n",
    "    Plots feature importances from a trained classifier.\n",
    "\n",
    "    Parameters:\n",
    "    - classifier: Trained model (e.g., DecisionTreeClassifier, RandomForestClassifier, LogisticRegressionClassifier).\n",
    "    - selected_features: List of RFE selected feature names corresponding to the model's input features.\n",
    "    \"\"\"\n",
    "    # Get the feature importances from the classifier\n",
    "    feature_importances = classifier.feature_importances_\n",
    "\n",
    "    # Create a dictionary of feature names and their corresponding importance values\n",
    "    importance_dict = dict(zip(selected_features, feature_importances))\n",
    "\n",
    "    # Sort the features by their importance values in descending order\n",
    "    sorted_importances = sorted(importance_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Print the sorted feature importances\n",
    "    print(\"\\nFeature Importances (sorted):\")\n",
    "    for feature, importance in sorted_importances:\n",
    "        print(f\"{feature}: {importance:.4f}\")\n",
    "\n",
    "    # Plotting feature importances\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Unpack the sorted features and their importance values\n",
    "    features, importances = zip(*sorted_importances)\n",
    "\n",
    "    # Create a bar plot to visualize the feature importances\n",
    "    plt.bar(features, importances, color='skyblue')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title('Feature Importances')\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Importance Score')\n",
    "\n",
    "    # Adjust the layout to prevent clipping of labels\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddaea17a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
